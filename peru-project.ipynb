{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1zMBgaz0ueO"
   },
   "source": [
    "# **Peru Project**- Replication Project\n",
    "\n",
    "---\n",
    "\n",
    "We are a group of 5 people. Group participants: \n",
    "1. Syed Shahvaiz Ahmed 18-756-635\n",
    "2. Georgios Anagnostou 20-741-260\n",
    "3. Vasiliki Arpatzoglou 18-747-980\n",
    "4. Santiago Cepeda 12-741-385\n",
    "5. Alex Wolf 12-526-216\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9EB-BScqvfN"
   },
   "source": [
    "## Introduction\n",
    "This project tries to replicate the paper from R. Hanna and B. A. Olken: \"Universal basic incomes versus targeted transfers: Anti-poverty programs in developing countries.\"\n",
    "\n",
    "The replication process was conducted in the same manner as in the paper. Thus, splitting the data according to the \"training\" attribute on the dataset. We also used imputation to fill missing values in the dataset.\n",
    "\n",
    "We then used different models to predict whether or not a household should receive benefits. Last but not least, we replicated Figure 5 with our CRRA-utility function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCYx4awq1gx4"
   },
   "source": [
    "## Uploading Relevant Libraries\n",
    "In this cell we upload the relevant libraries needed for the below cells.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "O9k6kpHl0WOK",
    "outputId": "476d757e-7f9f-4db1-9484-9279dfed8c32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# General math, csv and plotting modules.\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Widget and formatting modules\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual\n",
    "import pylab\n",
    "import seaborn as sns\n",
    "pylab.rcParams['figure.figsize'] = (10, 5)\n",
    "\n",
    "# Visualization Libraries\n",
    "import io\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plotly.subplots import make_subplots\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Loading the required libraries \n",
    "from fancyimpute import IterativeImputer as MICE\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from yellowbrick.classifier import DiscriminationThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Avoiding warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtDfhTYx55Az"
   },
   "source": [
    "## Setting Path\n",
    "Setting the path for the data file and reading the csv as pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdtnZs6J4ctK",
    "outputId": "018158c3-c566-43fa-ce7b-57334aad70d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzfKBAFxzkZx"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/peru_matlab_export_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rwoSj3T1kMd",
    "outputId": "d1c83cd4-e472-44e3-ba6d-52319a208c89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46305, 81)"
      ]
     },
     "execution_count": 171,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvSf3Q6M1mLc"
   },
   "source": [
    "The dataframe has **46305** rows and **81** columns. We have **73** variables (look at the appendix https://www.aeaweb.org/content/file?id=8344). The other 8 variables are related to previous matlab encodings and OLS estimators, which we do not need in this step. \n",
    "\n",
    "We want to **regress monthly household per-capita consumption** on the 72 actual indicator variables used in each country’s respective proxy-means test formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "ueYWIob33dmN",
    "outputId": "92c1df4f-0829-4101-b0c1-fc78fbd110e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lnpercapitaconsumption</th>\n",
       "      <th>d_fuel_other</th>\n",
       "      <th>d_fuel_wood</th>\n",
       "      <th>d_fuel_coal</th>\n",
       "      <th>d_fuel_kerosene</th>\n",
       "      <th>d_fuel_gas</th>\n",
       "      <th>d_fuel_electric</th>\n",
       "      <th>d_fuel_none</th>\n",
       "      <th>d_water_other</th>\n",
       "      <th>d_water_river</th>\n",
       "      <th>d_water_well</th>\n",
       "      <th>d_water_truck</th>\n",
       "      <th>d_water_pylon</th>\n",
       "      <th>d_water_outside</th>\n",
       "      <th>d_water_inside</th>\n",
       "      <th>d_drain_none</th>\n",
       "      <th>d_drain_river</th>\n",
       "      <th>d_drain_cesspool</th>\n",
       "      <th>d_drain_septic</th>\n",
       "      <th>d_drain_outside</th>\n",
       "      <th>d_drain_inside</th>\n",
       "      <th>d_wall_other</th>\n",
       "      <th>d_wall_woodmat</th>\n",
       "      <th>d_wall_stonemud</th>\n",
       "      <th>d_wall_quincha</th>\n",
       "      <th>d_wall_tapia</th>\n",
       "      <th>d_wall_adobe</th>\n",
       "      <th>d_wall_stonecement</th>\n",
       "      <th>d_wall_brickcement</th>\n",
       "      <th>d_roof_other</th>\n",
       "      <th>d_roof_straw</th>\n",
       "      <th>d_roof_mat</th>\n",
       "      <th>d_roof_platecane</th>\n",
       "      <th>d_roof_tile</th>\n",
       "      <th>d_roof_wood</th>\n",
       "      <th>d_roof_concrete</th>\n",
       "      <th>d_floor_other</th>\n",
       "      <th>d_floor_earth</th>\n",
       "      <th>d_floor_cement</th>\n",
       "      <th>d_floor_wood</th>\n",
       "      <th>...</th>\n",
       "      <th>d_floor_sheets</th>\n",
       "      <th>d_floor_parquet</th>\n",
       "      <th>d_electricity</th>\n",
       "      <th>d_telephone</th>\n",
       "      <th>d_h_educ_none</th>\n",
       "      <th>d_h_educ_pre</th>\n",
       "      <th>d_h_educ_prim</th>\n",
       "      <th>d_h_educ_sec</th>\n",
       "      <th>d_h_educ_higher_nouni</th>\n",
       "      <th>d_h_educ_higher_uni</th>\n",
       "      <th>d_h_educ_post</th>\n",
       "      <th>d_max_educ_none</th>\n",
       "      <th>d_max_educ_prim</th>\n",
       "      <th>d_max_educ_sec</th>\n",
       "      <th>d_max_educ_higher_nouni</th>\n",
       "      <th>d_max_educ_higher_uni</th>\n",
       "      <th>d_insurance_0</th>\n",
       "      <th>d_insurance_1</th>\n",
       "      <th>d_insurance_2</th>\n",
       "      <th>d_insurance_3</th>\n",
       "      <th>d_insurance_4plus</th>\n",
       "      <th>d_crowd_lessthan1</th>\n",
       "      <th>d_crowd_1to2</th>\n",
       "      <th>d_crowd_2to4</th>\n",
       "      <th>d_crowd_4to6</th>\n",
       "      <th>d_crowd_6plus</th>\n",
       "      <th>d_lux_0</th>\n",
       "      <th>d_lux_1</th>\n",
       "      <th>d_lux_2</th>\n",
       "      <th>d_lux_3</th>\n",
       "      <th>d_lux_4</th>\n",
       "      <th>d_lux_5</th>\n",
       "      <th>training</th>\n",
       "      <th>percapitaconsumption</th>\n",
       "      <th>poor</th>\n",
       "      <th>h_hhsize</th>\n",
       "      <th>id_for_matlab</th>\n",
       "      <th>hhid</th>\n",
       "      <th>lncaphat_OLS</th>\n",
       "      <th>percapitahat_OLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.352</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>233710511</td>\n",
       "      <td>5.246</td>\n",
       "      <td>284.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>320.139</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>295508011</td>\n",
       "      <td>6.129</td>\n",
       "      <td>522.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>390.832</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>257600211</td>\n",
       "      <td>6.666</td>\n",
       "      <td>878.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.602</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>198104311</td>\n",
       "      <td>6.088</td>\n",
       "      <td>567.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.771</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118.071</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>24805311</td>\n",
       "      <td>5.008</td>\n",
       "      <td>115.493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lnpercapitaconsumption  d_fuel_other  ...  lncaphat_OLS  percapitahat_OLS\n",
       "0                   5.352             0  ...         5.246           284.424\n",
       "1                   5.769             0  ...         6.129           522.884\n",
       "2                   5.968             0  ...         6.666           878.496\n",
       "3                   5.655             0  ...         6.088           567.471\n",
       "4                   4.771             0  ...         5.008           115.493\n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFR2tRF1ScZ1"
   },
   "source": [
    "## Train Test Split\n",
    "\n",
    "We split to train and test data according to the training column. We divide the testing and training data exactly as was mentioned on project description. If training number is 1 we keep it in the training set and if the training number is 0 we keep it in the test set. \n",
    "\n",
    "We directly keep as response variable the *log-transformed* capital consumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9m4b2ScL2jY"
   },
   "outputs": [],
   "source": [
    "train_data = df.loc[lambda df: df['training'] > 0, :]\n",
    "train_data = train_data.iloc[:,:73]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzrDkB6aNmNr",
    "outputId": "6fbd164f-9c9a-4328-b706-808949fbee91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23153, 72)"
      ]
     },
     "execution_count": 174,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data.iloc[:,1:]\n",
    "y_train = train_data.iloc[:,0]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhrwcjzWJCnY"
   },
   "outputs": [],
   "source": [
    "test_data = df.loc[lambda df: df['training'] == 0, :]\n",
    "comparative_metric = test_data['lncaphat_OLS'] # The actual y's in the test_data I believe.\n",
    "test_data = test_data.iloc[:,:73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gk9rLDsTJX2v",
    "outputId": "f2ce45e5-c6fb-4e25-fff6-0c4e2a1cff6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23152, 72)"
      ]
     },
     "execution_count": 177,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test_data.iloc[:,1:]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz9by4X9MoBn"
   },
   "source": [
    "## Imputation to fill missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGrKyp18VIek"
   },
   "source": [
    "This section is where we used a MICE imputer to impute the missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qi3cmULKh9D1",
    "outputId": "69ef20c0-6ed2-4226-8ea3-e513867f824a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46305, 72)"
      ]
     },
     "execution_count": 178,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_set = pd.concat([X_train, X_test], sort = False)\n",
    "merged_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WH_uqfai8VN"
   },
   "outputs": [],
   "source": [
    "# Imputing the missing values in the whole dataset\n",
    "merged_set_Imp = MICE().fit_transform(merged_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ls-_QiRk_bd"
   },
   "outputs": [],
   "source": [
    "merged_set_Imp = pd.DataFrame(merged_set_Imp)\n",
    "merged_set_Imp.columns = X_test.columns.values # renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSNhReNZMna3",
    "outputId": "72f5fa8b-e37f-4516-880b-f72a5adf4d20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23153, 72)"
      ]
     },
     "execution_count": 181,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the train set\n",
    "X_train_Imp = merged_set_Imp.iloc[:len(X_train)]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWxTTW5fMm1A",
    "outputId": "81465150-8f36-4b6b-ad07-5c312fdb7855"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23152, 72)"
      ]
     },
     "execution_count": 182,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the test set\n",
    "X_test_Imp = merged_set_Imp.iloc[len(X_train):]\n",
    "X_test_Imp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjbTkE8n6H-T"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5TZ6zasSw4g"
   },
   "source": [
    "This is where we define various machine learning models to get the relevant predictions. Since we know that the data distribution needs to be learned and at the very start we are not sure as to which model performs the best. In order to find that out, we tested the models mentioned below:\n",
    "\n",
    "1. Lasso\n",
    "2. Elastic Net\n",
    "3. Kernel Ridge Regression\n",
    "4. Gradient Boost (GB)\n",
    "5. XGBoost (XGB)\n",
    "6. Light Gradient Boost (LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjR0wWuMOSTn"
   },
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "lasso = make_pipeline(StandardScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "ENet = make_pipeline(StandardScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "KRR = KernelRidge(alpha=0.6, kernel=\"polynomial\", degree=2, coef0=2.5)\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_rJxIOLTGwK"
   },
   "source": [
    "We tested all the models separately and found out that GBoost, XGBoost, Gradient Boost performs the best. But we still were not satisfied with their individual performance so we coded an ensemble learner which basically takes the average prediction of all the three models and it turns out that it performed much better than any singular model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81YuY8nl7mgi"
   },
   "source": [
    "## Stacked Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEprdsXNUDND"
   },
   "source": [
    "In this class, each model is separately trained and predictions are averaged of all base models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DU7gnREd7qiZ"
   },
   "outputs": [],
   "source": [
    "class Stackedmodels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # Original models used for fitting data\n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        # Train specified models\n",
    "        for model in self.models_:\n",
    "            model.fit(X_train, Y_train)\n",
    "        return self\n",
    "    \n",
    "    # Predict on test data and take average of models\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNBlA_piIaRL"
   },
   "outputs": [],
   "source": [
    "# Validation function\n",
    "n_folds = 5\n",
    "# Spits the rmse of the model\n",
    "def rmsle_cv(model, x_train, y_train):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train)\n",
    "    rmse= np.sqrt(-cross_val_score(model, x_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9385YiLs9VBB"
   },
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrrfjCoC9Uad",
    "outputId": "41a19edd-9ae0-43d4-bb19-b3741553e8ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stackedmodels(models=(LGBMRegressor(bagging_fraction=0.8, bagging_freq=5,\n",
       "                                    bagging_seed=9, boosting_type='gbdt',\n",
       "                                    class_weight=None, colsample_bytree=1.0,\n",
       "                                    feature_fraction=0.2319,\n",
       "                                    feature_fraction_seed=9,\n",
       "                                    importance_type='split', learning_rate=0.05,\n",
       "                                    max_bin=55, max_depth=-1,\n",
       "                                    min_child_samples=20,\n",
       "                                    min_child_weight=0.001, min_data_in_leaf=6,\n",
       "                                    min_split_gain=0.0,\n",
       "                                    min_su...\n",
       "                                   colsample_bylevel=1, colsample_bynode=1,\n",
       "                                   colsample_bytree=0.4603, gamma=0.0468,\n",
       "                                   importance_type='gain', learning_rate=0.05,\n",
       "                                   max_delta_step=0, max_depth=3,\n",
       "                                   min_child_weight=1.7817, missing=None,\n",
       "                                   n_estimators=2200, n_jobs=1, nthread=-1,\n",
       "                                   objective='reg:linear', random_state=7,\n",
       "                                   reg_alpha=0.464, reg_lambda=0.8571,\n",
       "                                   scale_pos_weight=1, seed=None, silent=1,\n",
       "                                   subsample=0.5213, verbosity=1)))"
      ]
     },
     "execution_count": 187,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model list\n",
    "model_list = [lasso, model_lgb, ENet, KRR, GBoost, model_xgb]\n",
    "model_name = [\"lasso\", \"model_lgb\", \"ENet\", \"KRR\", \"GBoost\", \"model_xgb\"]\n",
    "\n",
    "# Model fit\n",
    "averaged_models = Stackedmodels(models = (model_lgb, GBoost, model_xgb))\n",
    "averaged_models.fit(X_train_Imp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caTGfV2fIeX-",
    "outputId": "a42f8a64-3555-4e06-c451-2587bf6c15b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['d_fuel_other', 'd_fuel_wood', 'd_fuel_coal', 'd_fuel_kerosene',\n",
       "       'd_fuel_gas', 'd_fuel_electric', 'd_fuel_none', 'd_water_other',\n",
       "       'd_water_river', 'd_water_well', 'd_water_truck', 'd_water_pylon',\n",
       "       'd_water_outside', 'd_water_inside', 'd_drain_none', 'd_drain_river',\n",
       "       'd_drain_cesspool', 'd_drain_septic', 'd_drain_outside',\n",
       "       'd_drain_inside', 'd_wall_other', 'd_wall_woodmat', 'd_wall_stonemud',\n",
       "       'd_wall_quincha', 'd_wall_tapia', 'd_wall_adobe', 'd_wall_stonecement',\n",
       "       'd_wall_brickcement', 'd_roof_other', 'd_roof_straw', 'd_roof_mat',\n",
       "       'd_roof_platecane', 'd_roof_tile', 'd_roof_wood', 'd_roof_concrete',\n",
       "       'd_floor_other', 'd_floor_earth', 'd_floor_cement', 'd_floor_wood',\n",
       "       'd_floor_tile', 'd_floor_sheets', 'd_floor_parquet', 'd_electricity',\n",
       "       'd_telephone', 'd_h_educ_none', 'd_h_educ_pre', 'd_h_educ_prim',\n",
       "       'd_h_educ_sec', 'd_h_educ_higher_nouni', 'd_h_educ_higher_uni',\n",
       "       'd_h_educ_post', 'd_max_educ_none', 'd_max_educ_prim', 'd_max_educ_sec',\n",
       "       'd_max_educ_higher_nouni', 'd_max_educ_higher_uni', 'd_insurance_0',\n",
       "       'd_insurance_1', 'd_insurance_2', 'd_insurance_3', 'd_insurance_4plus',\n",
       "       'd_crowd_lessthan1', 'd_crowd_1to2', 'd_crowd_2to4', 'd_crowd_4to6',\n",
       "       'd_crowd_6plus', 'd_lux_0', 'd_lux_1', 'd_lux_2', 'd_lux_3', 'd_lux_4',\n",
       "       'd_lux_5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 148,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_Imp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2aKKo9Y9T8C",
    "outputId": "398a9b9c-b555-4959-e3c2-d47c02491db5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23152"
      ]
     },
     "execution_count": 188,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the predictions \n",
    "y_pred = averaged_models.predict(X_test_Imp)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHejHz81pVDE",
    "outputId": "b6c9a7ee-afb2-446f-b439-8fb638e9e6c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7011171547590656"
      ]
     },
     "execution_count": 189,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the R square of the model\n",
    "averaged_models.score(X_train_Imp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ROXLwAFJfAaV",
    "outputId": "4681e9fa-6682-4f42-e9e6-738b6a18060a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lncaphat_OLS</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.246</td>\n",
       "      <td>5.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.129</td>\n",
       "      <td>6.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.666</td>\n",
       "      <td>6.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.088</td>\n",
       "      <td>6.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.008</td>\n",
       "      <td>4.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23147</th>\n",
       "      <td>5.060</td>\n",
       "      <td>5.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23148</th>\n",
       "      <td>5.082</td>\n",
       "      <td>5.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23149</th>\n",
       "      <td>5.212</td>\n",
       "      <td>4.970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23150</th>\n",
       "      <td>6.946</td>\n",
       "      <td>7.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23151</th>\n",
       "      <td>5.949</td>\n",
       "      <td>5.961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22704 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       lncaphat_OLS  y_pred\n",
       "0             5.246   5.228\n",
       "1             6.129   6.065\n",
       "2             6.666   6.652\n",
       "3             6.088   6.025\n",
       "4             5.008   4.945\n",
       "...             ...     ...\n",
       "23147         5.060   5.020\n",
       "23148         5.082   5.030\n",
       "23149         5.212   4.970\n",
       "23150         6.946   7.001\n",
       "23151         5.949   5.961\n",
       "\n",
       "[22704 rows x 2 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a dataframe\n",
    "bingo_df = pd.concat([comparative_metric, pd.Series(y_pred).rename('y_pred')], axis=1, sort = False)\n",
    "bingo_df = bingo_df[bingo_df['lncaphat_OLS'].notna()] # removing rows which don't report any values\n",
    "bingo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BA2WsO6cc_At",
    "outputId": "f1b5004d-a7ae-429d-d949-5a1ab9a918c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10308290004334325\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(bingo_df['lncaphat_OLS'], bingo_df['y_pred']))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-5cydZ3zzrl"
   },
   "source": [
    "## Figure 5 - Calculate CRRA-utility function\n",
    "The CRRA-utility function uses given values such as the national number of households, program budget, and UBI cost-savings bonus from the R. Hanna paper to calculate the utility values with our generated predictions. \n",
    "\n",
    "We calculated the CRRA-utility values by using the following formula:\n",
    "\n",
    "$$\n",
    "U = \\frac {\\sum(y_i + b_i)^{1-p}}{1-p}\n",
    "$$\n",
    "\n",
    "In order to calculate the inclusion error we iterate over all generated quantiles and defining which samples are included in the cut-off and not actually poor.\n",
    "\n",
    "For the last quantile / cut-off a bonus (2.235/12) for the UBI is added. This is done in order to simulate the cost-savings from administrative targeting at the UBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUD7ETFS4PsO"
   },
   "outputs": [],
   "source": [
    "def crra_utility(df):\n",
    "  data = df\n",
    "  national_num_households = 6750000\n",
    "  program_budget_monthly =  880000000/12\n",
    "  bonus_perhh = 2.235/12 # bonus in event of UBI\n",
    "  CRRA_TBFMS = np.zeros((101,1))\n",
    "  INC_ERR_TBFMS = np.zeros((101,1))\n",
    "  samplesize = sum(data['y'])\n",
    "\n",
    "  # Loop over inclusion errors in the training sample\n",
    "  c = []\n",
    "  for i in range(101):\n",
    "    c.append(np.quantile(data['y'],i/100))\n",
    "\n",
    "  incl_error_normal = []\n",
    "  for i, i_value in enumerate(c):\n",
    "    data['incl_c'] = (df['y'] <= i_value).astype(int)\n",
    "    incl_error_normal = []\n",
    "\n",
    "    for x,x_val in  enumerate(data['incl_c']):\n",
    "      if x_val==1 and data['poor'][x]==0: # inclusion error = poor but receiving money! [Switched from 1 to 0]\n",
    "        incl_error_normal.append(1)\n",
    "      else:\n",
    "        incl_error_normal.append(0) \n",
    "\n",
    "    \n",
    "    data['incl_error_normal'] = incl_error_normal\n",
    "    households_incl = sum(data['incl_c'])\n",
    "    pct_households_incl = households_incl / samplesize\n",
    "    national_hh_incl = national_num_households * pct_households_incl\n",
    "    \n",
    "    per_hh_benefits = program_budget_monthly / national_hh_incl\n",
    "    benefits_received = data['incl_c'] * per_hh_benefits\n",
    "    if i==100:\n",
    "      benefits_received += bonus_perhh\n",
    "    percapita_benefits_received = benefits_received / data['h_hhsize']\n",
    "    income_TBFMS = np.exp(data['y']) + percapita_benefits_received\n",
    "    CRRA_TBFMS[i] = sum(pow( income_TBFMS ,-2))/-2\n",
    "    \n",
    "    tp = data['incl_error_normal'].sum()\n",
    "    fp = sum(1-data['poor'])\n",
    "\n",
    "    INC_ERR_TBFMS[i] = tp/fp\n",
    "\n",
    "  return CRRA_TBFMS, INC_ERR_TBFMS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h993KfR-RfYD",
    "outputId": "e9545134-fa30-495b-cfc1-3d7a92da61a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23152, 81)\n"
     ]
    }
   ],
   "source": [
    "input = df[df['training']==0]\n",
    "print(input.shape)\n",
    "\n",
    "input['y'] = y_pred \n",
    "CRRA_TBFMS, INC_ERR_TBFMS = crra_utility(input )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PS7NoQJSQSYk",
    "outputId": "af3bad73-491f-47fc-933c-529a5c930874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest CRRA utility value at incl. error: [[0.09877654]]\n"
     ]
    }
   ],
   "source": [
    "print('Highest CRRA utility value at incl. error:',INC_ERR_TBFMS[pd.DataFrame(CRRA_TBFMS).idxmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4qsONM_P3RE"
   },
   "source": [
    "## Figure 5\n",
    "Since our prediction accuracy is higher than the one used in the R. Hanna paper we get lower utility function values. Thus, a better utility value.\n",
    "\n",
    "Our predictions show that the highest utility is achieved at an inclusion error of approximately 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "5T34jDds0CMU",
    "outputId": "c6a989a1-88e4-4557-c428-437c51060c5d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(INC_ERR_TBFMS,CRRA_TBFMS)\n",
    "plt.xlabel('Inclusion error')\n",
    "plt.ylabel('CRRA Utility - Peru')\n",
    "plt.title('Figure 5 - Social Welfare versus Inclusion Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "7O018heqEVRe",
    "outputId": "5944f002-229f-43f7-ab62-753589f8af40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lnpercapitaconsumption</th>\n",
       "      <th>d_fuel_other</th>\n",
       "      <th>d_fuel_wood</th>\n",
       "      <th>d_fuel_coal</th>\n",
       "      <th>d_fuel_kerosene</th>\n",
       "      <th>d_fuel_gas</th>\n",
       "      <th>d_fuel_electric</th>\n",
       "      <th>d_fuel_none</th>\n",
       "      <th>d_water_other</th>\n",
       "      <th>d_water_river</th>\n",
       "      <th>d_water_well</th>\n",
       "      <th>d_water_truck</th>\n",
       "      <th>d_water_pylon</th>\n",
       "      <th>d_water_outside</th>\n",
       "      <th>d_water_inside</th>\n",
       "      <th>d_drain_none</th>\n",
       "      <th>d_drain_river</th>\n",
       "      <th>d_drain_cesspool</th>\n",
       "      <th>d_drain_septic</th>\n",
       "      <th>d_drain_outside</th>\n",
       "      <th>d_drain_inside</th>\n",
       "      <th>d_wall_other</th>\n",
       "      <th>d_wall_woodmat</th>\n",
       "      <th>d_wall_stonemud</th>\n",
       "      <th>d_wall_quincha</th>\n",
       "      <th>d_wall_tapia</th>\n",
       "      <th>d_wall_adobe</th>\n",
       "      <th>d_wall_stonecement</th>\n",
       "      <th>d_wall_brickcement</th>\n",
       "      <th>d_roof_other</th>\n",
       "      <th>d_roof_straw</th>\n",
       "      <th>d_roof_mat</th>\n",
       "      <th>d_roof_platecane</th>\n",
       "      <th>d_roof_tile</th>\n",
       "      <th>d_roof_wood</th>\n",
       "      <th>d_roof_concrete</th>\n",
       "      <th>d_floor_other</th>\n",
       "      <th>d_floor_earth</th>\n",
       "      <th>d_floor_cement</th>\n",
       "      <th>d_floor_wood</th>\n",
       "      <th>...</th>\n",
       "      <th>d_h_educ_none</th>\n",
       "      <th>d_h_educ_pre</th>\n",
       "      <th>d_h_educ_prim</th>\n",
       "      <th>d_h_educ_sec</th>\n",
       "      <th>d_h_educ_higher_nouni</th>\n",
       "      <th>d_h_educ_higher_uni</th>\n",
       "      <th>d_h_educ_post</th>\n",
       "      <th>d_max_educ_none</th>\n",
       "      <th>d_max_educ_prim</th>\n",
       "      <th>d_max_educ_sec</th>\n",
       "      <th>d_max_educ_higher_nouni</th>\n",
       "      <th>d_max_educ_higher_uni</th>\n",
       "      <th>d_insurance_0</th>\n",
       "      <th>d_insurance_1</th>\n",
       "      <th>d_insurance_2</th>\n",
       "      <th>d_insurance_3</th>\n",
       "      <th>d_insurance_4plus</th>\n",
       "      <th>d_crowd_lessthan1</th>\n",
       "      <th>d_crowd_1to2</th>\n",
       "      <th>d_crowd_2to4</th>\n",
       "      <th>d_crowd_4to6</th>\n",
       "      <th>d_crowd_6plus</th>\n",
       "      <th>d_lux_0</th>\n",
       "      <th>d_lux_1</th>\n",
       "      <th>d_lux_2</th>\n",
       "      <th>d_lux_3</th>\n",
       "      <th>d_lux_4</th>\n",
       "      <th>d_lux_5</th>\n",
       "      <th>training</th>\n",
       "      <th>percapitaconsumption</th>\n",
       "      <th>poor</th>\n",
       "      <th>h_hhsize</th>\n",
       "      <th>id_for_matlab</th>\n",
       "      <th>hhid</th>\n",
       "      <th>lncaphat_OLS</th>\n",
       "      <th>percapitahat_OLS</th>\n",
       "      <th>y</th>\n",
       "      <th>incl_c</th>\n",
       "      <th>incl_error_normal</th>\n",
       "      <th>expY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.352</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>233710511</td>\n",
       "      <td>5.246</td>\n",
       "      <td>284.424</td>\n",
       "      <td>5.228</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>186.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.769</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>320.139</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>295508011</td>\n",
       "      <td>6.129</td>\n",
       "      <td>522.884</td>\n",
       "      <td>6.065</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>430.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>390.832</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>257600211</td>\n",
       "      <td>6.666</td>\n",
       "      <td>878.496</td>\n",
       "      <td>6.652</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>774.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.655</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.602</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>198104311</td>\n",
       "      <td>6.088</td>\n",
       "      <td>567.471</td>\n",
       "      <td>6.025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>413.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.771</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118.071</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>24805311</td>\n",
       "      <td>5.008</td>\n",
       "      <td>115.493</td>\n",
       "      <td>4.945</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140.539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   lnpercapitaconsumption  d_fuel_other  ...  incl_error_normal    expY\n",
       "0                   5.352             0  ...                  1 186.443\n",
       "1                   5.769             0  ...                  1 430.559\n",
       "2                   5.968             0  ...                  1 774.449\n",
       "3                   5.655             0  ...                  0 413.765\n",
       "4                   4.771             0  ...                  0 140.539\n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input['expY']=np.exp(input['y'])\n",
    "\n",
    "input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tG7nBscTAIhO"
   },
   "outputs": [],
   "source": [
    "# export results\n",
    "\n",
    "#“id_for_matlab”, “hhid”, “training”, “lnpercapitaconsumption “, “groupprediction”\n",
    "\n",
    "input.to_csv('group_predictions.csv', columns=['id_for_matlab','hhid','training','lnpercapitaconsumption','y','expY'])\n",
    "!cp group_predictions.csv \"drive/My Drive/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DbVqy4Qsuc2"
   },
   "source": [
    "## Conclusion\n",
    "As previously mentioned, we tested six seperate models and used the three best performing models: GBoost, XGBoost, Gradient Boost. However, the results could still be improved by using an ensemble learner, which took the average prediction of all three models. This approach achieved the best results. \n",
    "\n",
    "With the results from our ensemble learner and the CRRA-utility function we were able to replicate Figure 5 from the R. Hanna and B. A. Olken paper."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "peru.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
